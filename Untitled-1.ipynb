{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\k'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\k'\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_12676\\343041789.py:4: SyntaxWarning: invalid escape sequence '\\k'\n",
      "  df = pd.read_csv('kdt-NLANU-0.01.connlu.txt\\kdt-NLANU-0.01.connlu.txt',\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>WORD</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>POS</th>\n",
       "      <th>XPOS</th>\n",
       "      <th>MORPH</th>\n",
       "      <th>HEAD</th>\n",
       "      <th>DEPREL</th>\n",
       "      <th>DEPS</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ҚТЖ</td>\n",
       "      <td>ҚТЖ</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>_</td>\n",
       "      <td>4</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>халықаралық</td>\n",
       "      <td>халықаралық</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>_</td>\n",
       "      <td>3</td>\n",
       "      <td>amod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>серіктестікті</td>\n",
       "      <td>серіктестік</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Case=Acc</td>\n",
       "      <td>4</td>\n",
       "      <td>dobj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>кеңейтуде</td>\n",
       "      <td>кеңей</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Person=3|vbTense=Aor|vbVcCaus=True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>_</td>\n",
       "      <td>2</td>\n",
       "      <td>compound</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>мың</td>\n",
       "      <td>мың</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NUM</td>\n",
       "      <td>_</td>\n",
       "      <td>3</td>\n",
       "      <td>nummod</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>босқынға</td>\n",
       "      <td>босқын</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Case=Dat</td>\n",
       "      <td>5</td>\n",
       "      <td>iobj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>есік</td>\n",
       "      <td>есік</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>_</td>\n",
       "      <td>5</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           WORD        LEMMA    POS   XPOS  \\\n",
       "1   1            ҚТЖ          ҚТЖ  PROPN  PROPN   \n",
       "2   2    халықаралық  халықаралық    ADJ    ADJ   \n",
       "3   3  серіктестікті  серіктестік   NOUN   NOUN   \n",
       "4   4      кеңейтуде        кеңей   VERB   VERB   \n",
       "6   1            160          160    NUM    NUM   \n",
       "7   2            мың          мың    NUM    NUM   \n",
       "8   3       босқынға       босқын   NOUN   NOUN   \n",
       "9   4           есік         есік   NOUN   NOUN   \n",
       "\n",
       "                                MORPH HEAD    DEPREL DEPS MISC  \n",
       "1                                   _    4     nsubj    _    _  \n",
       "2                                   _    3      amod    _    _  \n",
       "3                            Case=Acc    4      dobj    _    _  \n",
       "4  Person=3|vbTense=Aor|vbVcCaus=True    0      root    _    _  \n",
       "6                                   _    2  compound    _    _  \n",
       "7                                   _    3    nummod    _    _  \n",
       "8                            Case=Dat    5      iobj    _    _  \n",
       "9                                   _    5     nsubj    _    _  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"ID\", \"WORD\", \"LEMMA\", \"POS\", \"XPOS\", \"MORPH\", \"HEAD\", \"DEPREL\", \"DEPS\", \"MISC\"]\n",
    "\n",
    "# Read the file and convert it to a DataFrame\n",
    "df = pd.read_csv('kdt-NLANU-0.01.connlu.txt\\kdt-NLANU-0.01.connlu.txt', \n",
    "                 sep='\\t', \n",
    "                 names=columns, \n",
    "                 skip_blank_lines=True)\n",
    "\n",
    "# Drop rows where 'WORD' is NaN\n",
    "df = df.dropna(subset=['WORD'])\n",
    "\n",
    "for col in df.columns:\n",
    "    try :\n",
    "        df[col] = df[col].str.strip()\n",
    "    except:\n",
    "        pass\n",
    "df['ID'] = df['ID'].astype(int)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "df.head(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "df_ = pd.DataFrame(df.head(100))\n",
    "\n",
    "# Initialize the sentence list and a temporary list for building the sentence\n",
    "sentence_list = []\n",
    "POS_list = []\n",
    "loc=0\n",
    "current_sentence = ''\n",
    "POS_tags = ''\n",
    "\n",
    "# Loop through each row to build sentences\n",
    "for index, row in df_.iterrows():\n",
    "    if row['ID'] == 1 :  # When ID == 1, a new sentence starts\n",
    "        loc += 1\n",
    "        sentence_list.append(current_sentence)\n",
    "        POS_list.append(POS_tags)\n",
    "\n",
    "        POS_tags = row['POS']\n",
    "        current_sentence = row['WORD']\n",
    "\n",
    "    else:\n",
    "        current_sentence = current_sentence + ' ' + row['WORD']\n",
    "        POS_tags = POS_tags + ' ' + row['POS']\n",
    "\n",
    "# Append the last sentence if it exists\n",
    "if current_sentence:\n",
    "    sentence_list.append(current_sentence.strip())\n",
    "    POS_list.append(POS_tags.strip())\n",
    "\n",
    "# Create a DataFrame with the 'sentence' column\n",
    "df_sentences = pd.DataFrame({\n",
    "    'sentence': sentence_list,\n",
    "    'POS': POS_list\n",
    "}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ҚТЖ халықаралық серіктестікті кеңейтуде</td>\n",
       "      <td>PROPN ADJ NOUN VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160 мың босқынға есік ашылды</td>\n",
       "      <td>NUM NUM NOUN NOUN VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Қазақ халқының басты жетістіктерінің бірі бай ...</td>\n",
       "      <td>NOUN NOUN ADJ NOUN PRON ADJ ADJ NOUN PUNCT NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Нұржанның үйленгеніне бес жыл болып еді .</td>\n",
       "      <td>PROPN NOUN NUM NOUN VERB AUX PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ол қоралар өзімен немере , шөберелес ағайындар...</td>\n",
       "      <td>PRON NOUN PRON NOUN PUNCT NOUN NOUN PUNCT PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Қартқожа сампылдап , ауылнай ішіндегі « адам »...</td>\n",
       "      <td>PROPN VERB PUNCT NOUN ADJ PUNCT NOUN PUNCT PUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Қазақстан көшбасшысының пікірінше , аталған тұ...</td>\n",
       "      <td>PROPN NOUN ADV PUNCT VERB NOUN NOUN PROPN PROP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Бір шарбақтың түбіне отырып дем алды .</td>\n",
       "      <td>PRON NOUN NOUN VERB NOUN VERB PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>БҰҰ Қауіпсіздік</td>\n",
       "      <td>PROPN NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0                                                      \n",
       "1            ҚТЖ халықаралық серіктестікті кеңейтуде   \n",
       "2                       160 мың босқынға есік ашылды   \n",
       "3  Қазақ халқының басты жетістіктерінің бірі бай ...   \n",
       "4          Нұржанның үйленгеніне бес жыл болып еді .   \n",
       "5  Ол қоралар өзімен немере , шөберелес ағайындар...   \n",
       "6  Қартқожа сампылдап , ауылнай ішіндегі « адам »...   \n",
       "7  Қазақстан көшбасшысының пікірінше , аталған тұ...   \n",
       "8             Бір шарбақтың түбіне отырып дем алды .   \n",
       "9                                    БҰҰ Қауіпсіздік   \n",
       "\n",
       "                                                 POS  \n",
       "0                                                     \n",
       "1                                PROPN ADJ NOUN VERB  \n",
       "2                             NUM NUM NOUN NOUN VERB  \n",
       "3  NOUN NOUN ADJ NOUN PRON ADJ ADJ NOUN PUNCT NOU...  \n",
       "4                 PROPN NOUN NUM NOUN VERB AUX PUNCT  \n",
       "5  PRON NOUN PRON NOUN PUNCT NOUN NOUN PUNCT PRON...  \n",
       "6  PROPN VERB PUNCT NOUN ADJ PUNCT NOUN PUNCT PUN...  \n",
       "7  PROPN NOUN ADV PUNCT VERB NOUN NOUN PROPN PROP...  \n",
       "8                PRON NOUN NOUN VERB NOUN VERB PUNCT  \n",
       "9                                         PROPN NOUN  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(df_) : \n",
    "    \n",
    "    df_ = df_.copy() # copy the input df\n",
    "\n",
    "    # Grouping by ID and aggregating the words and POS tags\n",
    "    df_['sentence'] = df_.groupby((df_['ID'] == 1).cumsum())['WORD'].transform(lambda x: ' '.join(x))\n",
    "    df_['POS'] = df_.groupby((df_['ID'] == 1).cumsum())['POS'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "    # Filter out rows where ID is 1, as they are not part of the final sentences\n",
    "    df_sentences = df_[df_['ID'] != 1][['sentence', 'POS']].reset_index(drop=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_sentences = df_sentences.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return df_sentences\n",
    "df_sentences = get_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ҚТЖ халықаралық серіктестікті кеңейтуде</td>\n",
       "      <td>PROPN ADJ NOUN VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160 мың босқынға есік ашылды</td>\n",
       "      <td>NUM NUM NOUN NOUN VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Қазақ халқының басты жетістіктерінің бірі бай ...</td>\n",
       "      <td>NOUN NOUN ADJ NOUN PRON ADJ ADJ NOUN PUNCT NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Нұржанның үйленгеніне бес жыл болып еді .</td>\n",
       "      <td>PROPN NOUN NUM NOUN VERB AUX PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ол қоралар өзімен немере , шөберелес ағайындар...</td>\n",
       "      <td>PRON NOUN PRON NOUN PUNCT NOUN NOUN PUNCT PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25988</th>\n",
       "      <td>« Салық , кеден , қылмыстық және әкімшілік кед...</td>\n",
       "      <td>PUNCT NOUN PUNCT NOUN PUNCT ADJ CONJ NOUN NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25989</th>\n",
       "      <td>Ас-су ішіп Әбдірахман екеуіміз үйден шыққанда ...</td>\n",
       "      <td>NOUN VERB PROPN NUM NOUN VERB NOUN NOUN VERB A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25990</th>\n",
       "      <td>Ол “ ЕАЭО Ресей мүддесіне көбірек жұмыс істейд...</td>\n",
       "      <td>PRON PUNCT PROPN PROPN NOUN ADV NOUN VERB VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25991</th>\n",
       "      <td>Ұлттық экономика министрі Ерболат Досаев Қазақ...</td>\n",
       "      <td>ADJ NOUN NOUN PROPN PROPN PROPN ADJ VERB ADJ V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25992</th>\n",
       "      <td>Аяқтап келгенде әңгіменің беті әйелге таман бұ...</td>\n",
       "      <td>VERB AUX NOUN NOUN NOUN ADP VERB PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25993 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0                ҚТЖ халықаралық серіктестікті кеңейтуде   \n",
       "1                           160 мың босқынға есік ашылды   \n",
       "2      Қазақ халқының басты жетістіктерінің бірі бай ...   \n",
       "3              Нұржанның үйленгеніне бес жыл болып еді .   \n",
       "4      Ол қоралар өзімен немере , шөберелес ағайындар...   \n",
       "...                                                  ...   \n",
       "25988  « Салық , кеден , қылмыстық және әкімшілік кед...   \n",
       "25989  Ас-су ішіп Әбдірахман екеуіміз үйден шыққанда ...   \n",
       "25990  Ол “ ЕАЭО Ресей мүддесіне көбірек жұмыс істейд...   \n",
       "25991  Ұлттық экономика министрі Ерболат Досаев Қазақ...   \n",
       "25992  Аяқтап келгенде әңгіменің беті әйелге таман бұ...   \n",
       "\n",
       "                                                     POS  \n",
       "0                                    PROPN ADJ NOUN VERB  \n",
       "1                                 NUM NUM NOUN NOUN VERB  \n",
       "2      NOUN NOUN ADJ NOUN PRON ADJ ADJ NOUN PUNCT NOU...  \n",
       "3                     PROPN NOUN NUM NOUN VERB AUX PUNCT  \n",
       "4      PRON NOUN PRON NOUN PUNCT NOUN NOUN PUNCT PRON...  \n",
       "...                                                  ...  \n",
       "25988  PUNCT NOUN PUNCT NOUN PUNCT ADJ CONJ NOUN NOUN...  \n",
       "25989  NOUN VERB PROPN NUM NOUN VERB NOUN NOUN VERB A...  \n",
       "25990  PRON PUNCT PROPN PROPN NOUN ADV NOUN VERB VERB...  \n",
       "25991  ADJ NOUN NOUN PROPN PROPN PROPN ADJ VERB ADJ V...  \n",
       "25992             VERB AUX NOUN NOUN NOUN ADP VERB PUNCT  \n",
       "\n",
       "[25993 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "df_ = pd.DataFrame(df.head(100))\n",
    "\n",
    "# Initialize the sentence list and a temporary list for building the sentence\n",
    "sentences = ['test']\n",
    "current_sentence = []\n",
    "loc=0\n",
    "\n",
    "# Loop through each row to build sentences\n",
    "for index, row in df_.iterrows():\n",
    "    if row['ID'] == 1 and current_sentence:  # When ID == 1 and it's not the first row, a new sentence starts\n",
    "        sentences.append(' '.join(current_sentence))  # Add the completed sentence to the list\n",
    "        current_sentence = []  # Start a new sentence\n",
    "\n",
    "    current_sentence.append(row['WORD'])  # Add the current word to the sentence\n",
    "\n",
    "# Append the last sentence if any\n",
    "if current_sentence:\n",
    "    sentences.append(' '.join(current_sentence))\n",
    "\n",
    "# Create a DataFrame with the 'sentence' column\n",
    "df_sentences = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ҚТЖ халықаралық серіктестікті кеңейтуде 160 мы...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0                                               test\n",
       "1  ҚТЖ халықаралық серіктестікті кеңейтуде 160 мы..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Word_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40408</th>\n",
       "      <td>vouchsafe</td>\n",
       "      <td>vouchsafing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10732</th>\n",
       "      <td>distillery</td>\n",
       "      <td>distilleries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10830</th>\n",
       "      <td>divine</td>\n",
       "      <td>divining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40767</th>\n",
       "      <td>weaving</td>\n",
       "      <td>weavings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29432</th>\n",
       "      <td>reboot</td>\n",
       "      <td>reboots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Lemma     Word_form\n",
       "40408   vouchsafe   vouchsafing\n",
       "10732  distillery  distilleries\n",
       "10830      divine      divining\n",
       "40767     weaving      weavings\n",
       "29432      reboot       reboots"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('lemmatization-en.txt', \n",
    "                 sep='\\t', \n",
    "                 header=None, \n",
    "                 names=['Lemma','Word_form']\n",
    "                 )\n",
    "df = df.sample(n=20)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemma'] = df['Lemma'].str.strip()\n",
    "df['Word_form'] = df['Word_form'].str.strip()\n",
    "\n",
    "df['Lemma'] = df['Lemma'].values\n",
    "df['Word_form'] = df['Word_form'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40408        vouchsafing\n",
       "10732       distilleries\n",
       "10830           divining\n",
       "40767           weavings\n",
       "29432            reboots\n",
       "38642         trap-doors\n",
       "13166        extraditing\n",
       "22037          meristems\n",
       "9267            deformed\n",
       "5762            chimerae\n",
       "11401              dupes\n",
       "7343           consortia\n",
       "17049       hexadecimals\n",
       "11924         embroiling\n",
       "36082             strove\n",
       "40234      vilifications\n",
       "22606        mislabeling\n",
       "21180            lustres\n",
       "38343              toted\n",
       "17432    horticulturists\n",
       "Name: Word_form, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Word_form']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chimerae', 'consortia', 'deformed', 'distilleries', 'divining',\n",
       "       'doors', 'dupes', 'embroiling', 'extraditing', 'hexadecimals',\n",
       "       'horticulturists', 'lustres', 'meristems', 'mislabeling',\n",
       "       'reboots', 'strove', 'toted', 'trap', 'vilifications',\n",
       "       'vouchsafing', 'weavings'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorizer = TfidfVectorizer()\n",
    "\n",
    "mx = Vectorizer.fit_transform(df['Word_form'])\n",
    "\n",
    "Vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\olivier\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\olivier\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\olivier\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 840.2 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 987.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#from sklearn.linear_model import LogisticRegression, LinearRegression, Lars, RidgeCV\n",
    "#from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, cross_val_score\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import gc\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['алматыға', 'аулада', 'балалар', 'барды', 'басталды', 'болып',\n",
       "       'біз', 'жүр', 'кітап', 'күн', 'мектепте', 'мен', 'ойнап', 'ол',\n",
       "       'отырмын', 'оқып', 'сабақ', 'тауға', 'тұр', 'шуақты', 'шығамыз'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_lang(lang_code):\n",
    "    ds=pd.read_csv('lang_codes_dic.csv',';')\n",
    "    try:\n",
    "        return ds[ds.Code==lang_code].iloc[0,1]\n",
    "    except:\n",
    "        return 'Undefined'\n",
    "\n",
    "\n",
    "#vec encoding of words\n",
    "def alpha_vec2(w, mx, max_word_len, dic):\n",
    "    vec=np.zeros((max_word_len,len(dic)))    \n",
    "    for i in range(0, len(w)):\n",
    "        #print(i,w[i])\n",
    "        vec[i]=mx[dic.index(w[i])]\n",
    "        \n",
    "    vec=vec.astype('float16').flatten()\n",
    "    vec[vec==np.inf]=0 \n",
    "    vec[vec==-np.inf]=0        \n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "#ordinal encoding of words\n",
    "def alpha_vec2ord(w, max_word_len):\n",
    "    vec=np.zeros(max_word_len)    \n",
    "    for i in range(0, len(w)):        \n",
    "        vec[i]=ord(w[i])    \n",
    "    return vec.astype('int')\n",
    "\n",
    "#ordinal decoding of words\n",
    "def decode_vec(vec):\n",
    "    w=''.join([chr(int(v)) for v in vec if v!=0])    \n",
    "    return w.strip()\n",
    "\n",
    "def lemm_model(dictionary_file):\n",
    "    #load data\n",
    "    lex = pd.read_csv('DS_lemm/'+ dictionary_file,'\\t', encoding='utf8', names=['Lemma','Word_form'], keep_default_na=False)\n",
    "    lang_code=dictionary_file[dictionary_file.index('-')+1:-4]\n",
    "    lang=get_lang(lang_code)\n",
    "    print('Language: ', lang, lang_code)\n",
    "    number_words=lex.shape[0]\n",
    "    \n",
    "    X_lex=lex['Word_form'].str.strip()\n",
    "    X_lex=X_lex.values\n",
    "    \n",
    "    Y_lex=lex['Lemma'].str.strip()\n",
    "    Y_lex=Y_lex.values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_lex, Y_lex, test_size=0.1, random_state=42)\n",
    "    \n",
    "    #get max word length\n",
    "    max_word_len=max(max([len(w) for w in Y_lex]),max([len(w) for w in X_lex]))\n",
    "    \n",
    "    #Char2vec model\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, analyzer='char')\n",
    "    X = vectorizer.fit_transform(X_lex)\n",
    "    dic=vectorizer.get_feature_names()#letter dictionary\n",
    "    num_letters=len(dic)\n",
    "    mx=X.T.dot(X)#letter cooccurence matrix\n",
    "    mx=mx.toarray()\n",
    "    \n",
    "    #Vectorize\n",
    "    X_lex_vec_train=[alpha_vec2(w, mx, max_word_len, dic) for w in X_train]\n",
    "    Y_lex_vec_train=[alpha_vec2ord(w, max_word_len) for w in y_train]\n",
    "    \n",
    "    X_lex_vec_test=[alpha_vec2(w, mx, max_word_len, dic) for w in X_test]\n",
    "    Y_lex_vec_test=[alpha_vec2ord(w, max_word_len) for w in y_test]\n",
    "    \n",
    "    #Build model\n",
    "    best_model=ExtraTreesClassifier(n_estimators=10, n_jobs=5, criterion='entropy', bootstrap=True)\n",
    "    \n",
    "    best_model.fit(X_lex_vec_train, Y_lex_vec_train)\n",
    "    \n",
    "    #Test\n",
    "    predicts_test=best_model.predict(X_lex_vec_test)\n",
    "    predicts_train=best_model.predict(X_lex_vec_train)\n",
    "    test_acc=sum([sum(p==y)==max_word_len for p,y in zip(predicts_test, Y_lex_vec_test)])/len(predicts_test)\n",
    "    train_acc=sum([sum(p==y)==max_word_len for p,y in zip(predicts_train, Y_lex_vec_train)])/len(predicts_train)\n",
    "    \n",
    "    #Return results\n",
    "    return test_acc, train_acc, max_word_len, num_letters, number_words, lang, lang_code, X_test, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
